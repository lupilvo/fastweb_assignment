{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank Term Deposit Acceptance forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obiettivo di questo assignment è la valutazione delle competenze tecniche del candidato e la sua modalità di approccio ad un problema di Data Science. Nello specifico è richiesto di sviluppare un modello predittivo in grado di indicare se un cliente intercettato da una campagna di marketing da parte di una banca decide di sottoscrivere o meno un deposito bancario a termine (bank term deposit).\n",
    "\n",
    "#### Dataset: ####\n",
    "\n",
    "All'interno della cartella **data**  viene fornito il file **bank-dataset.csv** che contiene le campagne marketing telefoniche effettuate da una banca per proporre l'acquisto del prodotto bancario.\n",
    "I dettagli del dataset sono forniti all'interno del file: **bank-names.txt**.\n",
    "La variabile target che indica se il cliente accetta o meno la sottoscrizione del deposito bancario è contenuta nel medesimo file con field name \"y\".\n",
    "\n",
    "#### Assignement: ####\n",
    "\n",
    "Richiesta di questo assignment è la costruzione di un modello predittivo con performance soddisfacenti per il candidato dando evidenza di tutti gli step tipici che dovrebbero essere affrontati in un progetto di Data Science: dalla pulizia e preparazione del dato fino al testing delle performance del modello costruito.\n",
    "\n",
    "Il notebook svolto dovrà essere opportunamente commentato e dovrà essere consegnato tramite condivisione di un repository github personale accessibile che ne permetta la riproduzione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library and custom funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_values(df, col, **kwargs):\n",
    "    \"\"\"\n",
    "    Function than wrap pandas value_counts functions in order to check multiple columns\n",
    "    :param df: input dataframe\n",
    "    :param col: selected column\n",
    "    :param **kwargs: pandas value_counts params\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nColumn: {col}\\n\")\n",
    "    print(df[col].value_counts(**kwargs))\n",
    "    print(\"#\"*100)\n",
    "    \n",
    "def normalize_col(df, col, nan_value=\"unknown\"):\n",
    "    \"\"\"\n",
    "    Apply lower case and replace nan value to a column\n",
    "    :param df: input dataframe\n",
    "    :param col: selected column\n",
    "    :param nan_value: value to replace in nan (default: 'unknown')\n",
    "    :return dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df[col] = df[col].str.lower()\n",
    "    df[col] = df[col].replace(np.nan, 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def replace_missing(df):\n",
    "    \"\"\"\n",
    "    Check and replace missing value with mode\n",
    "    :param df: input dataframe\n",
    "    :return dataframe\n",
    "    \"\"\"\n",
    "        \n",
    "    for col in df.columns:\n",
    "        isna = df[col].isna().values.any()\n",
    "        if isna:\n",
    "            new_value = df[col].mode()[0]\n",
    "            df[col] = df[col].fillna(new_value)\n",
    "            print(f\"Replaced {col} missing value with nan values: {new_value}\")   \n",
    "        \n",
    "    return df\n",
    "\n",
    "def plot_correlation_matrix(df, single_corner=False, target=None):\n",
    "    \"\"\"\n",
    "    Plot correlation matrix with option to convert target column and plot half matrix \n",
    "    :param df: input dataframe\n",
    "    :param single_corner: boolean to plot only half matrix (default: False)\n",
    "    :param target: name of target column to convert into numerical column (default: None)\n",
    "    \"\"\"\n",
    "    \n",
    "    df_corr = df.copy()\n",
    "    if target:\n",
    "        binary_map = {\"no\": 0, \"yes\": 1}\n",
    "        df_corr[target] = df_corr[target].map(binary_map)\n",
    "        \n",
    "    df_corr = df_corr.corr()    \n",
    "    if single_corner:\n",
    "        mask_corr = np.zeros_like(df_corr, dtype=bool)\n",
    "        mask_corr[np.triu_indices_from(mask_corr)] = True\n",
    "        df_corr[mask_corr] = np.nan\n",
    "        \n",
    "    sns.set(rc={'figure.figsize':(10,8)}, style='darkgrid')   \n",
    "    corr_matrix = sns.heatmap(df_corr, vmin=0, vmax=1, annot =True, cmap=\"YlGnBu\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_object_col(df, col, target=\"y\"):\n",
    "    \"\"\"\n",
    "    Plot bivariate histogram with col values and target value\n",
    "    :param df: input dataframe\n",
    "    :param col: selected column\n",
    "    :param target : name of target column (default: 'y')\n",
    "    \"\"\"\n",
    "    \n",
    "    sns.set(rc={'figure.figsize':(14,10)}, style='darkgrid')\n",
    "    \n",
    "    colors = [\"coral\", \"lightgreen\"]\n",
    "    sns.set_palette(sns.color_palette(colors))\n",
    "    \n",
    "    plt_col = sns.countplot(x=col, data = df, hue = target, order = df[col].value_counts().index)\n",
    "    \n",
    "    plt_col.tick_params(axis='x', rotation=50)\n",
    "    plt.title(f\"Relationship between {col} and {target}\",fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "def plot_numerical_col(df,col,target='y'):\n",
    "    \"\"\"\n",
    "    Plot bivariate boxplot and histogram with col values and target value with mean ,mode, median values\n",
    "    :param df: input dataframe\n",
    "    :param col: selected column\n",
    "    :param target : name of target column (default: 'y')\n",
    "    \"\"\"\n",
    "    \n",
    "    mean=df[col].mean()\n",
    "    median=df[col].median()\n",
    "    mode=df[col].mode().values[0]\n",
    "    \n",
    "    f, (box, hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.3, 1)})\n",
    "    sns.set(rc={'figure.figsize':(14,10)}, style='darkgrid')\n",
    "    \n",
    "    colors = [\"coral\", \"lightgreen\"]\n",
    "    sns.set_palette(sns.color_palette(colors))\n",
    "    \n",
    "    sns.boxplot(data=df, x=col, y=target, ax=box, order = df[target].value_counts().index)\n",
    "    sns.histplot(data=df, x=col, kde=True, ax=hist)\n",
    "    \n",
    "    box.axvline(mean, color='r')\n",
    "    box.axvline(median, color='g')\n",
    "    box.axvline(mode, color='b')\n",
    "    box.set(xlabel='')\n",
    "    box.set_title(f\"Relationship between {col} and {target}\",fontsize=18)\n",
    "\n",
    "    hist.axvline(mean, color='r', label=\"Mean\")\n",
    "    hist.axvline(median, color='g', label=\"Median\")\n",
    "    hist.axvline(mode, color='b', label=\"Mode\")\n",
    "    hist.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "def plot_confusion_matrix(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    :param y_pred: predicted values\n",
    "    :param y_true: true value\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "    sns.set(rc={'figure.figsize':(8,6)}, style='darkgrid')   \n",
    "    sns.heatmap(cm, annot=True,fmt='g',cmap=\"YlGnBu\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_shap_waterfall_plot(shap_values, features, num_display=20):\n",
    "    \n",
    "    '''\n",
    "    A function for building a SHAP waterfall plot.\n",
    "    \n",
    "    SHAP waterfall plot is used to visualize the most important features in a descending order.\n",
    "    \n",
    "    Parameters:\n",
    "    shap_values (list): SHAP values obtained from a model\n",
    "    features (pandas DataFrame): a list of features used in a model\n",
    "    num_display(int): number of features to display\n",
    "    \n",
    "    Returns:\n",
    "    matplotlib.pyplot plot: SHAP waterfall plot\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    column_list = features.columns\n",
    "    feature_ratio = (np.abs(shap_values).sum(0) / np.abs(shap_values).sum()) * 100\n",
    "    column_list = column_list[np.argsort(feature_ratio)[::-1]]\n",
    "    feature_ratio_order = np.sort(feature_ratio)[::-1]\n",
    "    cum_sum = np.cumsum(feature_ratio_order)\n",
    "    column_list = column_list[:num_display]\n",
    "    feature_ratio_order = feature_ratio_order[:num_display]\n",
    "    cum_sum = cum_sum[:num_display]\n",
    "    \n",
    "    num_height = 0\n",
    "    if (num_display >= 20) & (len(column_list) >= 20):\n",
    "        num_height = (len(column_list) - 20) * 0.4\n",
    "        \n",
    "    fig, ax1 = plt.subplots(figsize=(8, 8 + num_height))\n",
    "    ax1.plot(cum_sum[::-1], column_list[::-1], c='blue', marker='o')\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.barh(column_list[::-1], feature_ratio_order[::-1], alpha=0.6)\n",
    "    \n",
    "    ax1.grid(True)\n",
    "    ax2.grid(False)\n",
    "    ax1.set_xticks(np.arange(0, round(cum_sum.max(), -1)+1, 10))\n",
    "    ax2.set_xticks(np.arange(0, round(feature_ratio_order.max(), -1)+1, 10))\n",
    "    ax1.set_xlabel('Cumulative Ratio')\n",
    "    ax2.set_xlabel('Composition Ratio')\n",
    "    ax1.tick_params(axis=\"y\", labelsize=13)\n",
    "    plt.ylim(-1, len(column_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Exploraty data analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/bank-dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check values for all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = list(df.select_dtypes(include=\"object\").columns)\n",
    "num_cols = list(df.select_dtypes(exclude=\"object\").columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Object values\n",
    "\n",
    "La variabile target è molto sbilanciata sulla classe no-> 93% vs 7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in obj_cols:\n",
    "    check_values(df,col,normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize string column with lowercase and replace NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in obj_cols:\n",
    "    df = normalize_col(df,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix wrong values for marital values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_single = df[\"marital\"].str.startswith(\"s\")\n",
    "mask_divorced = df[\"marital\"].str.startswith(\"d\")\n",
    "\n",
    "df.loc[mask_divorced, \"marital\"] = \"divorced\"\n",
    "df.loc[mask_single, \"marital\"] = \"single\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check null/nan values \n",
    "\n",
    "Le colonne **age, duration** hanno dei valori mancanti. I valori mancanti verrà sostituiti con le relative mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = replace_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "Non ci sono features correlate fra di loro e neanche con la variabile target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_correlation_matrix(df, single_corner=True, target=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    plot_numerical_col(df,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in obj_cols:\n",
    "    plot_object_col(df,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features selection\n",
    "\n",
    "Dai precendenti plot risulta opportuno eliminare le seguenti features: **month, day, pdays, previous**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"month\", \"day\", \"pdays\", \"previous\"]\n",
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert binary cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = [\"default\",\"housing\",\"loan\",\"y\"]\n",
    "binary_map = {\"no\": 0, \"yes\": 1}\n",
    "\n",
    "for col in binary_cols:\n",
    "    df[col] = df[col].map(binary_map)\n",
    "    obj_cols.remove(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert categorical cols (OneHotEncoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['job', 'marital', 'contact', 'education', 'poutcome']\n",
    "df = pd.get_dummies(df, columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"y\"]\n",
    "X = df.drop(\"y\",axis = 1)\n",
    "\n",
    "sm = SMOTE()\n",
    "X_sm , y_sm = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X_sm, y_sm, test_size = 0.2, random_state = 123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model \n",
    "Effettuo il training con una GridSearchCV per fare il tuning di alcuni hyperparametri utilizzando la CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [None, 50],\n",
    "    \"max_features\" : ['auto', 'sqrt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict result on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutte le metriche hanno dei valori alti, dunque il modello addestrato riesce a classificare correttamente entrambe le classi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain model with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10 #Prendiamo solamente 10 esempi per velocizzare l'esecuzione\n",
    "samples = X_test.reset_index(drop=True)[0:n_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(samples, approximate=False, check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il grafico sopra mostra l'impattto sulla predizione delle principali variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_shap_waterfall_plot(shap_values[1], samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il grafico sopra mostra l'impatto percentuale delle principali variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_features = ['duration','poutcome_unknown','marital_married','housing','contact_unknown']\n",
    "for feature in top_features:\n",
    "    shap.dependence_plot(feature, shap_values[1], samples, interaction_index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast_env",
   "language": "python",
   "name": "fast_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
